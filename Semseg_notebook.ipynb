{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Semantic Segmentation - Inria Intership 2019 \n",
    "![alt text](https://www.inria.fr/var/inria/storage/images/medias/recherche/images-chapo/l-informatique-de-a-a-z/cryptographie-chapo/73800-2-eng-GB/cryptographie-chapo1_abecedaire_epi.jpg \"Chroma Team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repository\n",
    "Fork of [meetshah1995's PyTorch Semantic Segmentation](https://github.com/meetshah1995/pytorch-semseg)\n",
    "\n",
    "Let's import all the needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import shutil\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, import some tools from the **pysemseg directory**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptsemseg.models import get_model\n",
    "from ptsemseg.loss import get_loss_function\n",
    "from ptsemseg.loader import get_loader\n",
    "from ptsemseg.utils import get_logger\n",
    "from ptsemseg.metrics import runningScore, averageMeter\n",
    "from ptsemseg.augmentations import get_composed_augmentations\n",
    "from ptsemseg.schedulers import get_scheduler\n",
    "from ptsemseg.optimizers import get_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create the Train method which takes the YAML config file and then proceeds to train the network according to the parameters given such as:\n",
    "* Network architecture\n",
    "* Dataset\n",
    "* Training parameters (epochs, batch size, optimizer, loss)\n",
    "* Output based on the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, writer, logger):\n",
    "\n",
    "    # Setup seeds\n",
    "    torch.manual_seed(cfg.get(\"seed\", 1337))\n",
    "    torch.cuda.manual_seed(cfg.get(\"seed\", 1337))\n",
    "    np.random.seed(cfg.get(\"seed\", 1337))\n",
    "    random.seed(cfg.get(\"seed\", 1337))\n",
    "\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Setup Augmentations\n",
    "    augmentations = cfg[\"training\"].get(\"augmentations\", None)\n",
    "    data_aug = get_composed_augmentations(augmentations)\n",
    "\n",
    "    # Setup Dataloader\n",
    "    data_loader = get_loader(cfg[\"data\"][\"dataset\"])\n",
    "    data_path = cfg[\"data\"][\"path\"]\n",
    "\n",
    "    t_loader = data_loader(\n",
    "        data_path,\n",
    "        is_transform=True,\n",
    "        split=cfg[\"data\"][\"train_split\"],\n",
    "        img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n",
    "        augmentations=data_aug,\n",
    "    )\n",
    "\n",
    "    v_loader = data_loader(\n",
    "        data_path,\n",
    "        is_transform=True,\n",
    "        split=cfg[\"data\"][\"val_split\"],\n",
    "        img_size=(cfg[\"data\"][\"img_rows\"], cfg[\"data\"][\"img_cols\"]),\n",
    "    )\n",
    "\n",
    "    n_classes = t_loader.n_classes\n",
    "    trainloader = data.DataLoader(\n",
    "        t_loader,\n",
    "        batch_size=cfg[\"training\"][\"batch_size\"],\n",
    "        num_workers=cfg[\"training\"][\"n_workers\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    valloader = data.DataLoader(\n",
    "        v_loader, batch_size=cfg[\"training\"][\"batch_size\"], num_workers=cfg[\"training\"][\"n_workers\"]\n",
    "    )\n",
    "\n",
    "    # Setup Metrics\n",
    "    running_metrics_val = runningScore(n_classes)\n",
    "\n",
    "    # Setup Model\n",
    "    model = get_model(cfg[\"model\"], n_classes).to(device)\n",
    "\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "    # Setup optimizer, lr_scheduler and loss function\n",
    "    optimizer_cls = get_optimizer(cfg)\n",
    "    optimizer_params = {k: v for k, v in cfg[\"training\"][\"optimizer\"].items() if k != \"name\"}\n",
    "\n",
    "    optimizer = optimizer_cls(model.parameters(), **optimizer_params)\n",
    "    logger.info(\"Using optimizer {}\".format(optimizer))\n",
    "\n",
    "    scheduler = get_scheduler(optimizer, cfg[\"training\"][\"lr_schedule\"])\n",
    "\n",
    "    loss_fn = get_loss_function(cfg)\n",
    "    logger.info(\"Using loss {}\".format(loss_fn))\n",
    "\n",
    "    start_iter = 0\n",
    "    if cfg[\"training\"][\"resume\"] is not None:\n",
    "        if os.path.isfile(cfg[\"training\"][\"resume\"]):\n",
    "            logger.info(\n",
    "                \"Loading model and optimizer from checkpoint '{}'\".format(cfg[\"training\"][\"resume\"])\n",
    "            )\n",
    "            checkpoint = torch.load(cfg[\"training\"][\"resume\"])\n",
    "            model.load_state_dict(checkpoint[\"model_state\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "            start_iter = checkpoint[\"epoch\"]\n",
    "            logger.info(\n",
    "                \"Loaded checkpoint '{}' (iter {})\".format(\n",
    "                    cfg[\"training\"][\"resume\"], checkpoint[\"epoch\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"No checkpoint found at '{}'\".format(cfg[\"training\"][\"resume\"]))\n",
    "\n",
    "    val_loss_meter = averageMeter()\n",
    "    time_meter = averageMeter()\n",
    "\n",
    "    best_iou = -100.0\n",
    "    i = start_iter\n",
    "    flag = True\n",
    "\n",
    "    while i <= cfg[\"training\"][\"train_iters\"] and flag:\n",
    "        for (images, labels) in trainloader:\n",
    "            i += 1\n",
    "            start_ts = time.time()\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            time_meter.update(time.time() - start_ts)\n",
    "\n",
    "            if (i + 1) % cfg[\"training\"][\"print_interval\"] == 0:\n",
    "                fmt_str = \"Iter [{:d}/{:d}]  Loss: {:.4f}  Time/Image: {:.4f}\"\n",
    "                print_str = fmt_str.format(\n",
    "                    i + 1,\n",
    "                    cfg[\"training\"][\"train_iters\"],\n",
    "                    loss.item(),\n",
    "                    time_meter.avg / cfg[\"training\"][\"batch_size\"],\n",
    "                )\n",
    "\n",
    "                print(print_str)\n",
    "                logger.info(print_str)\n",
    "                writer.add_scalar(\"loss/train_loss\", loss.item(), i + 1)\n",
    "                time_meter.reset()\n",
    "\n",
    "            if (i + 1) % cfg[\"training\"][\"val_interval\"] == 0 or (i + 1) == cfg[\"training\"][\n",
    "                \"train_iters\"\n",
    "            ]:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n",
    "                        images_val = images_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "\n",
    "                        outputs = model(images_val)\n",
    "                        val_loss = loss_fn(input=outputs, target=labels_val)\n",
    "\n",
    "                        pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "                        gt = labels_val.data.cpu().numpy()\n",
    "\n",
    "                        running_metrics_val.update(gt, pred)\n",
    "                        val_loss_meter.update(val_loss.item())\n",
    "\n",
    "                writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n",
    "                logger.info(\"Iter %d Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n",
    "\n",
    "                score, class_iou = running_metrics_val.get_scores()\n",
    "                for k, v in score.items():\n",
    "                    print(k, v)\n",
    "                    logger.info(\"{}: {}\".format(k, v))\n",
    "                    writer.add_scalar(\"val_metrics/{}\".format(k), v, i + 1)\n",
    "\n",
    "                for k, v in class_iou.items():\n",
    "                    logger.info(\"{}: {}\".format(k, v))\n",
    "                    writer.add_scalar(\"val_metrics/cls_{}\".format(k), v, i + 1)\n",
    "\n",
    "                val_loss_meter.reset()\n",
    "                running_metrics_val.reset()\n",
    "\n",
    "                if score[\"Mean IoU : \\t\"] >= best_iou:\n",
    "                    best_iou = score[\"Mean IoU : \\t\"]\n",
    "                    state = {\n",
    "                        \"epoch\": i + 1,\n",
    "                        \"model_state\": model.state_dict(),\n",
    "                        \"optimizer_state\": optimizer.state_dict(),\n",
    "                        \"scheduler_state\": scheduler.state_dict(),\n",
    "                        \"best_iou\": best_iou,\n",
    "                    }\n",
    "                    save_path = os.path.join(\n",
    "                        writer.file_writer.get_logdir(),\n",
    "                        \"{}_{}_best_model.pkl\".format(cfg[\"model\"][\"arch\"], cfg[\"data\"][\"dataset\"]),\n",
    "                    )\n",
    "                    torch.save(state, save_path)\n",
    "\n",
    "            if (i + 1) == cfg[\"training\"][\"train_iters\"]:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"config\")\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        nargs=\"?\",\n",
    "        type=str,\n",
    "        default=\"configs/fcn8s_pascal.yml\",\n",
    "        help=\"Configuration file to use\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.config) as fp:\n",
    "        cfg = yaml.load(fp)\n",
    "\n",
    "    run_id = random.randint(1, 100000)\n",
    "    logdir = os.path.join(\"runs\", os.path.basename(args.config)[:-4], str(run_id))\n",
    "    writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "    print(\"RUNDIR: {}\".format(logdir))\n",
    "    shutil.copy(args.config, logdir)\n",
    "\n",
    "    logger = get_logger(logdir)\n",
    "    logger.info(\"Let the games begin\")\n",
    "\n",
    "train(cfg, writer, logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
